% Use only LaTeX2e, calling the article.cls class and 12-point type.
\documentclass[12pt]{article}
\usepackage{amsmath} 
\numberwithin{table}{section}
\usepackage{longtable}
\usepackage{biblatex}

% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.0cm
\textwidth 16cm
\textheight 20cm
\footskip 3.0cm

% Include your paper's title here
\title{\vspace{-4.5cm}AI Tetris - A CS3243 Project (Group 21)}
\author{Lau Kar Rui (A0155936U), Poh Jie, Matilda (A0178867A)}
% Start of document.
\begin{document} 
% 1.75 space the manuscript.
\baselineskip17.5pt
% Make the title.
\maketitle 

\section{Introduction}
In this project, we create an utility based Tetris-playing AI Agent with utility weights derived through the \emph{Particle Swarm Optimization} algorithm to maximise the number of cleared rows in a single game. A standard Tetris board of 20 rows by 10 columns is used, along with the standard 7 tetrominos (I, O, T, S, Z, J, and L).

The best move is chosen by the \emph{state} that maximizes the utility function. This is done by the summation of the values of 8 different features. Calculating the summation of all the features is trivial, but deciding the best weights for our features to pick the best move is both complex and crucial for our agent to perform to its highest potential. 

\section{Utility Function}\label{util-function}
We defined the utility function as a linear function $F$ with each feature $f_i \in Features$ having an assigned weight $w_i$, where $i = 1...n$, with $n$ being the number of features. Each $f_i$ derives a real value from a state $s$. The function is then defined as:
	$$F(s) = \sum_{i = 1}^{n} w_if_i(s)$$
\section{Features Used} \label{features}
Table \ref{featurestable} shows the features $f_i$ used to calculate $w_i \in Weights$ when training with the PSO algorithm described in Section \ref{PSO}. 

\begin{longtable}[H]{|p{4cm}|p{11.2cm}|}
\hline
\textbf{Feature}  & \textbf{Description}                                                                                                                                \\ \hline
RowsCleared       & Number of rows cleared                                                                                                                              \\ \hline
MaxHeightIncrease & The maximum height increase among all columns                                                                                                       \\ \hline
AvgHeightIncrease & The average height increase among all columns                                                                                                       \\ \hline
AbsoluteDiff      & The absolute height difference between all columns                                                                                                  \\ \hline
NumHoles          & Number of holes; a hole is defined as an empty cell with a non-empty cell above it                                                                  \\ \hline
ColumnTransition  & The total number of column transitions; a column transition is defined as an empty cell adjacent to a filled cell on the same column and vice versa \\ \hline
RowTransition     & The total number of row transitions; a row transition is defined as an empty cell adjacent to a filled cell on the same row and vice versa          \\ \hline
WellSum           & The total number of empty cells above the columns' top heights which are adjacent to filled cells on both sides                                     \\ \hline
\caption{Features used}
\label{featurestable}
\end{longtable}

\section{Implementation}
The utility function $F$ is applied for every possible state $s_i$ using the \texttt{StateCopy} mechanism described in Section \ref{statecopy} (with state meaning every possible orientation and landing column of the current landing piece), and the $s_i$ with $\max \left\lbrace value(F(s_i)) \mid \forall s_i\in States\right\rbrace$ is chosen to be the current state.
\subsection{StateCopy} \label{statecopy}
In order to correctly apply the features, a new \texttt{StateCopy} class was created, extending the original \texttt{State} class, serving as a clean starting state to apply our features on in order to derive the feature value.

Extra variables like \texttt{currentRowsCleared} and \texttt{previousTop} is also added to the \texttt{StateCopy} class in order to obtain the information needed by various features such as the \emph{RowsCleared} and the \emph{AverageHeight} features.

Using \texttt{StateCopy} also allows us to play moves without affecting the original state of the game to find the move with the highest function value.

\subsection{Particle Swarm Optimization (PSO)} \label{PSO}
The PSO algorithm is inspired by the metaphor of social interaction observed between fishes or birds.
The basic variant of PSO is used, with a population (a.k.a. swarm) of candidate solutions (a.k.a. particles), which moves around in the search-space of our linear function $F$, guided by their own best known position in the search-space as well as the entire swarm's best known position to find the best positions (a.k.a. $w_i \in Weights$) for $F(s)$.

The movement of each particle $\vec{x}_i$ for $i=1...N$, where $N$ is the number of particles in the swarm, is guided by the formulae
\begin{gather*}
\vec{v}\leftarrow \omega\vec{v} + \phi_p r_p ( \vec{p} - \vec{x} ) + \phi_g r_g ( \vec{g} - \vec{x} )
\\
\vec{x}\leftarrow \vec{x} + \vec{v}
\end{gather*}
where $\vec{p}$ is the particle's \emph{personal best position} and $\vec{g}$ is the swarm's \emph{global best position}, $\phi_p$ and $\phi_g$ being the \emph{social} and \emph{cognitive coefficient} respectively, emulating the swarm's willingness to move in the search-space. 

$\phi_p$ acts as the particle's ``memory'', causing it to return to its individual best regions of the search space, while $\phi_g$ guides the particle to move to the localized search-space where the global best position was discovered. $r_p$ and $r_g$ are two random real values $\in\left[0..1\right]$ generated every assignment to $\vec{v}$. 

$\omega$ refers to the \emph{inertia} of the particle, and keeps the particle moving in the same direction it was originally heading. A lower value speeds up convergence of the swarm in the search space, and a higher value encourages exploring the search-space.

Values for these parameters were first chosen based on \cite{1} and later optimized with the Meta Optimization algorithm discussed in Section \ref{meta-opt}.

\subsection{Meta Optimization of PSO using PSO}\label{meta-opt}
Talk about the PSO optimization here

\section{Scaling The Algorithm For Big Data}
Talk about parallelizing the application here
Talk about using Compute Cluster

\section{Training Results}\label{weights}
After 1000 iterations of our training algorithm described in Section \ref{PSO}, we arrived at the weights:
$$
\left( \begin{array}{c} 
MaxHeightIncrease
\\RowsCleared
\\AvgHeightIncrease
\\NumHoles
\\ColumnTransition
\\AbsoluteDiff
\\RowTransition
\\WellSum
\end{array} \right)
\Longrightarrow\left( \begin{array}{c}
-2.9210777318332948
\\-1.3634453151532058
\\-6.350233933389025
\\-3.015076993292611
\\-7.867411559467035
\\-1.0057536655180186
\\-1.6874510272386336
\\-1.851236997094957
\end{array} \right)
$$
\\
It is interesting to see that the weights obtained for the $RowsCleared$ feature is negative, meaning the linear function $F$ minimizes the number of rows cleared instead of maximizing it as one would expect. This may be because it is riskier to allow a greater height without clearing any rows and is a byproduct of height minimization, as there are no incentives for the learning algorithm to allow for such risks (such as a score multiplier when multiple rows are cleared). 

\section{Results}
The results of 100 runs using the weights obtained in Section \ref{weights} can be seen in Fig. \ref{results-graph} below.

\section{Conclusion}



% Sample reference list for the various ways to reference.
\begin{thebibliography}{12}
\bibitem{pso-values-reference}
Vandenbergh, F., \& Engelbrecht, A. (2006).
\\A study of particle swarm optimization particle trajectories. Information Sciences, 176(8), 937-971. doi:10.1016/j.ins.2005.02.003
\end{thebibliography}

\end{document}